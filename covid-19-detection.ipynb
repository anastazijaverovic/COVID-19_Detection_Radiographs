{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem description\n\nFive times more deadly than the flu, COVID-19 causes significant morbidity and mortality. Like other pneumonias, pulmonary infection with COVID-19 results in inflammation and fluid in the lungs. COVID-19 looks very similar to other viral and bacterial pneumonias on chest radiographs, which makes it difficult to diagnose. This computer vision model for detection and localization of COVID-19 would help doctors provide a quick and confident diagnosis. As a result, patients could get the right treatment before the most severe effects of the virus take hold.\n\n\nCurrently, COVID-19 can be diagnosed via polymerase chain reaction to detect genetic material from the virus or chest radiograph. However, it can take a few hours and sometimes days before the molecular test results are back. By contrast, chest radiographs can be obtained in minutes. While guidelines exist to help radiologists differentiate COVID-19 from other types of infection, their assessments vary. In addition, non-radiologists could be supported with better localization of the disease, such as with a visual bounding box.\n\n\nIn this competition, the task is to identify and localize COVID-19 abnormalities on chest radiographs. In particular, categorization of the radiographs as negative for pneumonia or typical, indeterminate, or atypical for COVID-19.","metadata":{}},{"cell_type":"markdown","source":"**Categorization of the radiographs:**\n\n* NEGATIVE FOR PNEUMONIA - No lung opacities\n\n* TYPICAL APPEARANCE - Multifocal bilateral, peripheral opacities with rounded morphology, lower lung–predominant distribution\n\n* INDETERMINATE APPEARANCE - Absence of typical findings AND unilateral, central or upper lung predominant distribution\n\n* ATYPICAL APPEARANCE - Pneumothorax, pleural effusion, pulmonary edema, lobar consolidation, solitary lung nodule or mass, diffuse tiny nodules, cavity","metadata":{}},{"cell_type":"markdown","source":"**Input data:**\n\n* train_study_level.csv - the train study-level metadata, with one row for each study, including correct labels.\n* train_image_level.csv - the train image-level metadata, with one row for each image, including both correct labels and any bounding boxes in a dictionary format. Some images in both test and train have multiple bounding boxes.\n* sample_submission.csv - a sample submission file containing all image- and study-level IDs.\n* train folder - comprises 6334 chest scans in DICOM format, stored in paths with the form study/series/image\n* test folder - The hidden test dataset is of roughly the same scale as the training dataset. Studies in the test set may contain more than one label.","metadata":{}},{"cell_type":"markdown","source":"# Content table\n\n1. Importing the libraries\n2. Importing the datasets\n3. Data exploration\n4. Read Dicom files\n5. Feature engineering\n6. Making the model\n7. Compiling the model\n8. References","metadata":{}},{"cell_type":"markdown","source":"# Importing the libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sn\nimport pydicom as dicom # Dicom (Digital Imaging in Medicine) - medical image datasets, storage and transfer\nimport os\nfrom tqdm import tqdm # allows you to output a smart progress bar by wrapping around any iterable\nimport glob # retrieve files/pathnames matching a specified pattern\nimport pprint # pretty-print” arbitrary Python data structures\nimport ast # \nfrom pydicom.pixel_data_handlers.util import apply_voi_lut #\nimport wandb #\nfrom tensorflow as t\nimport efficientnet.tfkeras as efn\n\npd.set_option('display.max_columns', 500)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-14T10:49:23.913769Z","iopub.execute_input":"2021-06-14T10:49:23.914106Z","iopub.status.idle":"2021-06-14T10:49:26.006665Z","shell.execute_reply.started":"2021-06-14T10:49:23.914054Z","shell.execute_reply":"2021-06-14T10:49:26.005787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing the datasets","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/siim-covid19-detection/'\ntrain_image_level = pd.read_csv(path + \"train_image_level.csv\")\ntrain_study_level = pd.read_csv(path + \"train_study_level.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:26.008317Z","iopub.execute_input":"2021-06-14T10:49:26.008693Z","iopub.status.idle":"2021-06-14T10:49:26.066056Z","shell.execute_reply.started":"2021-06-14T10:49:26.008644Z","shell.execute_reply":"2021-06-14T10:49:26.064916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data exploration","metadata":{}},{"cell_type":"markdown","source":"Let's have a look inside the train_image_level.","metadata":{}},{"cell_type":"code","source":"train_image_level.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:26.068041Z","iopub.execute_input":"2021-06-14T10:49:26.068423Z","iopub.status.idle":"2021-06-14T10:49:26.091159Z","shell.execute_reply.started":"2021-06-14T10:49:26.068387Z","shell.execute_reply":"2021-06-14T10:49:26.090168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_level.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:26.093316Z","iopub.execute_input":"2021-06-14T10:49:26.093729Z","iopub.status.idle":"2021-06-14T10:49:26.14391Z","shell.execute_reply.started":"2021-06-14T10:49:26.09369Z","shell.execute_reply":"2021-06-14T10:49:26.14302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 6334 unique values in the train_image_level dataframe.","metadata":{}},{"cell_type":"code","source":"train_study_level.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:26.145131Z","iopub.execute_input":"2021-06-14T10:49:26.145459Z","iopub.status.idle":"2021-06-14T10:49:26.155643Z","shell.execute_reply.started":"2021-06-14T10:49:26.145423Z","shell.execute_reply":"2021-06-14T10:49:26.154642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_study_level.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:26.157145Z","iopub.execute_input":"2021-06-14T10:49:26.15773Z","iopub.status.idle":"2021-06-14T10:49:26.185965Z","shell.execute_reply.started":"2021-06-14T10:49:26.157677Z","shell.execute_reply":"2021-06-14T10:49:26.184877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 6054 rows in the train_study_level dataframe. The number of unique values in study dataframe differs from the unique values in the images dataframe. Let's check how many studies have more than 1 image linked.","metadata":{}},{"cell_type":"code","source":"train_study_level_key = train_study_level.id.str[:-6]\ntraining_set = pd.merge(left = train_study_level, right = train_image_level, how = 'right', left_on = train_study_level_key, right_on = 'StudyInstanceUID')\ntraining_set.drop(['id_x'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:26.187605Z","iopub.execute_input":"2021-06-14T10:49:26.188014Z","iopub.status.idle":"2021-06-14T10:49:26.223822Z","shell.execute_reply.started":"2021-06-14T10:49:26.187977Z","shell.execute_reply":"2021-06-14T10:49:26.222856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at these studies with multiple images:","metadata":{}},{"cell_type":"code","source":"training_set[training_set.groupby('StudyInstanceUID')['id_y'].transform('size') > 1].sort_values('StudyInstanceUID')","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:26.225323Z","iopub.execute_input":"2021-06-14T10:49:26.225889Z","iopub.status.idle":"2021-06-14T10:49:26.255372Z","shell.execute_reply.started":"2021-06-14T10:49:26.22583Z","shell.execute_reply":"2021-06-14T10:49:26.254404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Dicom files","metadata":{}},{"cell_type":"markdown","source":"Function used to locate image from the path:","metadata":{}},{"cell_type":"code","source":"def extract_image(i):\n    path_train = path + 'train/' + training_set.loc[i, 'StudyInstanceUID']\n    last_folder_in_path = os.listdir(path_train)[0]\n    path_train = path_train + '/{}/'.format(last_folder_in_path)\n    img_id = training_set.loc[i, 'id_y'].replace('_image','.dcm')\n    print(img_id)\n    data_file = dicom.dcmread(path_train + img_id)\n    img = data_file.pixel_array\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:26.258731Z","iopub.execute_input":"2021-06-14T10:49:26.259066Z","iopub.status.idle":"2021-06-14T10:49:26.26545Z","shell.execute_reply.started":"2021-06-14T10:49:26.259038Z","shell.execute_reply":"2021-06-14T10:49:26.264512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Images and rectangles visualization**","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3,3, figsize=(20,16))\nfig.subplots_adjust(hspace=.1, wspace=.1)\naxes = axes.ravel()\n\nfor row in range(9):\n    img = extract_image(row)\n    if (training_set.loc[row,'boxes'] == training_set.loc[row,'boxes']):\n        boxes = ast.literal_eval(training_set.loc[row,'boxes'])\n        for box in boxes:\n            p = matplotlib.patches.Rectangle((box['x'], box['y']),\n                                              box['width'], box['height'],\n                                              ec = 'r', fc = 'none', lw = 2.\n                                            )\n            axes[row].add_patch(p)\n    axes[row].imshow(img, cmap = 'gray')\n    axes[row].set_title(training_set.loc[row, 'label'].split(' ')[0])\n    axes[row].set_xticklabels([])\n    axes[row].set_yticklabels([])","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:26.267751Z","iopub.execute_input":"2021-06-14T10:49:26.268131Z","iopub.status.idle":"2021-06-14T10:49:37.367874Z","shell.execute_reply.started":"2021-06-14T10:49:26.268094Z","shell.execute_reply":"2021-06-14T10:49:37.366914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"**Opacity_Count** - Count the number of opacities in the image","metadata":{}},{"cell_type":"code","source":"Opacity_Count = training_set['label'].str.count('opacity')\ntraining_set['Opacity_Count'] = Opacity_Count.values","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:37.368986Z","iopub.execute_input":"2021-06-14T10:49:37.369356Z","iopub.status.idle":"2021-06-14T10:49:37.385213Z","shell.execute_reply.started":"2021-06-14T10:49:37.369319Z","shell.execute_reply":"2021-06-14T10:49:37.384329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Rectange_Area** - Sum of areas of rectangles - assumption : the bigger the rectangle - the bigger the opacity","metadata":{}},{"cell_type":"code","source":"image_rectangles_areas = []\n\nfor row in range(6334):#len(training_set.index)):\n    image_rectangles_area_sum = 0\n    rectangle_area = 0\n    if (training_set.loc[row,'boxes'] == training_set.loc[row,'boxes']):\n        boxes = ast.literal_eval(training_set.loc[row,'boxes'])\n        for box in boxes:\n            rectangle_area = box['width'] * box['height']\n            image_rectangles_area_sum = image_rectangles_area_sum + rectangle_area\n        image_rectangles_areas.append(image_rectangles_area_sum)\n    else:\n        image_rectangles_area_sum = image_rectangles_area_sum + rectangle_area\n        image_rectangles_areas.append(image_rectangles_area_sum)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:37.386362Z","iopub.execute_input":"2021-06-14T10:49:37.38697Z","iopub.status.idle":"2021-06-14T10:49:37.708418Z","shell.execute_reply.started":"2021-06-14T10:49:37.38693Z","shell.execute_reply":"2021-06-14T10:49:37.707454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set['Rectangle_Area'] = image_rectangles_areas","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:37.709728Z","iopub.execute_input":"2021-06-14T10:49:37.710091Z","iopub.status.idle":"2021-06-14T10:49:37.716218Z","shell.execute_reply.started":"2021-06-14T10:49:37.710042Z","shell.execute_reply":"2021-06-14T10:49:37.715305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating buckets - rectangle areas**","metadata":{}},{"cell_type":"markdown","source":"First see the distribution of the rectangle areas","metadata":{}},{"cell_type":"code","source":"training_set['Rectangle_Area'] = round(training_set['Rectangle_Area'],2)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:37.717437Z","iopub.execute_input":"2021-06-14T10:49:37.718006Z","iopub.status.idle":"2021-06-14T10:49:37.728763Z","shell.execute_reply.started":"2021-06-14T10:49:37.717959Z","shell.execute_reply":"2021-06-14T10:49:37.727851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set['Rectangle_Area']","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:37.730999Z","iopub.execute_input":"2021-06-14T10:49:37.73137Z","iopub.status.idle":"2021-06-14T10:49:37.744238Z","shell.execute_reply.started":"2021-06-14T10:49:37.731334Z","shell.execute_reply":"2021-06-14T10:49:37.74317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.qcut(training_set['Rectangle_Area'], q = 4)\n\n#training_set.boxplot(by = \"Negative for Pneumonia\",column = ['Rectangle_Area'],grid = True, layout=(1, 1))\n\ncut_labels_4 = ['0', '<1e6', '<2e6', '<4e6', '<8e6']\ncut_bins = [-1, 0, 1000000, 2000000, 4000000, 8000000]\ntraining_set['Rectangle_Area_Bin'] = pd.cut(training_set['Rectangle_Area'], bins=cut_bins, labels=cut_labels_4)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:37.747477Z","iopub.execute_input":"2021-06-14T10:49:37.747799Z","iopub.status.idle":"2021-06-14T10:49:37.756757Z","shell.execute_reply.started":"2021-06-14T10:49:37.74777Z","shell.execute_reply":"2021-06-14T10:49:37.755837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\n\nplt.figure(figsize = (16, 14))\nsn.set(font_scale = 1.2)\nsn.set_style('ticks')\n\nfor i, column in enumerate(columns):\n    plt.subplot(3, 3, i + 1)\n    sn.countplot(data = training_set, x = 'Rectangle_Area_Bin', hue = column, palette = ['#d02f52',\"#55a0ee\"])\n    \nsn.despine()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:37.758314Z","iopub.execute_input":"2021-06-14T10:49:37.758728Z","iopub.status.idle":"2021-06-14T10:49:38.348007Z","shell.execute_reply.started":"2021-06-14T10:49:37.75869Z","shell.execute_reply":"2021-06-14T10:49:38.346968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#columns = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\nopacity = sorted(list(training_set['Rectangle_Area_Bin'].value_counts().index))\n\nfor i in opacity:\n    Count_Series = training_set[training_set['Rectangle_Area_Bin'] == i].iloc[:,[1, 2, 3, 4]].sum()\n    fig = plt.figure(figsize=(12,3))\n    sn.barplot(x = Count_Series.index, y = Count_Series.values/sum(training_set['Rectangle_Area_Bin'] == i))\n    plt.title('Rectangle_Area_Bin : {} '.format(i))\n    plt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:38.3496Z","iopub.execute_input":"2021-06-14T10:49:38.34994Z","iopub.status.idle":"2021-06-14T10:49:38.934567Z","shell.execute_reply.started":"2021-06-14T10:49:38.349905Z","shell.execute_reply":"2021-06-14T10:49:38.933574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rectangle area and opacity count","metadata":{}},{"cell_type":"code","source":"#columns = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\nopacity = sorted(list(training_set['Opacity_Count'].value_counts().index))\n\nfor i in opacity:\n    Count_Series = training_set[training_set['Opacity_Count'] == i].iloc[:,[1, 2, 3, 4]].sum()\n    fig = plt.figure(figsize=(12,3))\n    sn.barplot(x = Count_Series.index, y = Count_Series.values/sum(training_set['Opacity_Count'] == i))\n    plt.title('OpacityCount : {} '.format(i))\n    plt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:38.935931Z","iopub.execute_input":"2021-06-14T10:49:38.936313Z","iopub.status.idle":"2021-06-14T10:49:39.730932Z","shell.execute_reply.started":"2021-06-14T10:49:38.936276Z","shell.execute_reply":"2021-06-14T10:49:39.729999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TBD**: Position of the rectangle by quadrants (4 bins - 4 quadrants)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Image metadata**","metadata":{}},{"cell_type":"code","source":"training_paths = []\ntrain_directory = \"../input/siim-covid19-detection/train/\"\n\nfor sid in tqdm(training_set['StudyInstanceUID']):\n    training_paths.append(glob.glob(os.path.join(train_directory, sid +\"/*/*\"))[0])\n\ntraining_set['path'] = training_paths","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:39.732327Z","iopub.execute_input":"2021-06-14T10:49:39.732671Z","iopub.status.idle":"2021-06-14T10:49:59.482529Z","shell.execute_reply.started":"2021-06-14T10:49:39.732633Z","shell.execute_reply":"2021-06-14T10:49:59.481171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pixel values are in the range of 0 to 255. It is easier for us to normalize the data between 0 to 1 and we can do that just by dividing our train and test set by 255.","metadata":{}},{"cell_type":"code","source":"voi_lut=True\nfix_monochrome=True\n\ndef dicom_dataset_to_dict(filename,func):\n    \"\"\"Credit: https://github.com/pydicom/pydicom/issues/319\n               https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    \"\"\"\n    \n    dicom_header = dicom.dcmread(filename) \n    \n    #====== DICOM FILE DATA ======\n    dicom_dict = {}\n    repr(dicom_header)\n    for dicom_value in dicom_header.values():\n        if dicom_value.tag == (0x7fe0, 0x0010):\n            #discard pixel data\n            continue\n        if type(dicom_value.value) == dicom.dataset.Dataset:\n            dicom_dict[dicom_value.name] = dicom_dataset_to_dict(dicom_value.value)\n        else:\n            v = _convert_value(dicom_value.value)\n            dicom_dict[dicom_value.name] = v\n      \n    del dicom_dict['Pixel Representation']\n    \n    if func != 'metadata_df':\n        #====== DICOM IMAGE DATA ======\n        # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n        if voi_lut:\n            data = apply_voi_lut(dicom_header.pixel_array, dicom_header)\n        else:\n            data = dicom_header.pixel_array\n        # depending on this value, X-ray may look inverted - fix that:\n        if fix_monochrome and dicom_header.PhotometricInterpretation == \"MONOCHROME1\":\n            data = np.amax(data) - data\n        data = data - np.min(data)\n        data = data / np.max(data)\n        modified_image_data = (data * 255).astype(np.uint8)\n    \n        return dicom_dict, modified_image_data\n    \n    else:\n        return dicom_dict\n\ndef _sanitise_unicode(s):\n    return s.replace(u\"\\u0000\", \"\").strip()\n\ndef _convert_value(v):\n    t = type(v)\n    if t in (list, int, float):\n        cv = v\n    elif t == str:\n        cv = _sanitise_unicode(v)\n    elif t == bytes:\n        s = v.decode('ascii', 'replace')\n        cv = _sanitise_unicode(s)\n    elif t == dicom.valuerep.DSfloat:\n        cv = float(v)\n    elif t == dicom.valuerep.IS:\n        cv = int(v)\n    else:\n        cv = repr(v)\n    return cv\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-14T10:49:59.485376Z","iopub.execute_input":"2021-06-14T10:49:59.485639Z","iopub.status.idle":"2021-06-14T10:49:59.497029Z","shell.execute_reply.started":"2021-06-14T10:49:59.485605Z","shell.execute_reply":"2021-06-14T10:49:59.496063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting the dictionary data to the dataframe and dropping the columns not needed","metadata":{}},{"cell_type":"code","source":"metadata = []\n\nfor filename in training_set.path:\n    try:\n        data_di = dicom_dataset_to_dict(filename,'metadata_df')\n        metadata.append(data_di)\n    except:\n        continue\n\ndicom_data_df = pd.DataFrame(metadata)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-14T10:49:59.498403Z","iopub.execute_input":"2021-06-14T10:49:59.499169Z","iopub.status.idle":"2021-06-14T11:04:20.365918Z","shell.execute_reply.started":"2021-06-14T10:49:59.499133Z","shell.execute_reply":"2021-06-14T11:04:20.365029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dicom_data_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-14T11:04:20.367445Z","iopub.execute_input":"2021-06-14T11:04:20.367784Z","iopub.status.idle":"2021-06-14T11:04:20.396151Z","shell.execute_reply.started":"2021-06-14T11:04:20.36775Z","shell.execute_reply":"2021-06-14T11:04:20.39523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dicom_data_df.drop(['Specific Character Set', 'SOP Class UID','SOP Instance UID','Study Date','Study Time','Accession Number','Patient ID','Accession Number','Rows','Columns'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-14T11:04:20.397436Z","iopub.execute_input":"2021-06-14T11:04:20.398Z","iopub.status.idle":"2021-06-14T11:04:20.436135Z","shell.execute_reply.started":"2021-06-14T11:04:20.39796Z","shell.execute_reply":"2021-06-14T11:04:20.435149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the metadata information as new columns in an existing dataframe","metadata":{}},{"cell_type":"code","source":"training_set_merged = pd.merge(left = training_set, right = dicom_data_df, how = 'left', left_on = 'StudyInstanceUID', right_on = 'Study Instance UID')\ntraining_set_merged","metadata":{"execution":{"iopub.status.busy":"2021-06-14T11:04:20.437243Z","iopub.execute_input":"2021-06-14T11:04:20.43764Z","iopub.status.idle":"2021-06-14T11:04:20.50831Z","shell.execute_reply.started":"2021-06-14T11:04:20.437604Z","shell.execute_reply":"2021-06-14T11:04:20.507602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TBD**: Outliers and irregularities in the data","metadata":{}},{"cell_type":"markdown","source":"# Building the model","metadata":{}},{"cell_type":"markdown","source":"EfficientNet is used:\n\nThe EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use  times more computational resources, then we can simply increase the network depth by , width by , and image size by , where  are constant coefficients determined by a small grid search on the original small model.\n\nThe compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.\n\nThe base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2, in addition to squeeze-and-excitation blocks.","metadata":{}},{"cell_type":"markdown","source":"**Layers:**\n\n\n**The pooling layer** operates upon each feature map separately to create a new set of the same number of pooled feature maps.\nPooling involves selecting a pooling operation, much like a filter to be applied to feature maps.\nAverage Pooling: Calculate the average value for each patch on the feature map.\n\n**Dense** implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\nDense is the only actual network layer in the model.\nA Dense layer feeds all outputs from the previous layer to all its neurons, each neuron providing one output to the next layer. A Dense(10) has ten neurons.\n\n**Model** groups layers into an object with training and inference features.\n\n**Adam** is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n\nLoss is a prediction error of Neural Net. And the method to calculate the loss is called Loss Function. In simple words, the Loss is used to calculate the gradients. And gradients are used to update the weights of the Neural Net.\n\n**CategoricalCrossentropy** - crossentropy loss function when there are two or more label classes","metadata":{}},{"cell_type":"code","source":"EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n\ndef build_model(dim = IMG_SIZES[0], ef = 0):\n    inputs = tf.keras.layers.Input(shape = (*dim, 3))\n    base = EFNS[ef](input_shape = (*dim,3),weights = 'imagenet',include_top = False)\n    x = base(inputs)\n    \n    # pooling layer\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    # Dense\n    x = tf.keras.layers.Dense(64, activation = 'relu')(x)\n    x = tf.keras.layers.Dense(4, activation = 'softmax')(x)\n    \n    model = tf.keras.Model(inputs = inputs, outputs = x)\n    \n    opt = tf.keras.optimizers.Adam(learning_rate = 0.001)\n    \n    loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing = 0.01)\n    \n    auc = tf.keras.metrics.AUC(curve = 'ROC', multi_label = True)\n    \n    acc = tf.keras.metrics.CategoricalAccuracy()\n    \n    f1  = tfa.metrics.F1Score(num_classes = 4,average = 'macro',threshold = None)\n    \n    model.compile(optimizer = opt,loss = loss,metrics = [auc, acc, f1])\n    \n    return model","metadata":{"execution":{"iopub.status.busy":"2021-06-14T11:04:20.509437Z","iopub.execute_input":"2021-06-14T11:04:20.509909Z","iopub.status.idle":"2021-06-14T11:04:20.513504Z","shell.execute_reply.started":"2021-06-14T11:04:20.509873Z","shell.execute_reply":"2021-06-14T11:04:20.512433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n* https://github.com/pydicom/pydicom/issues/319\n* https://www.kaggle.com/songseungwon/siim-covid-19-detection-10-step-tutorial-1\n* https://www.kaggle.com/ruchi798/siim-covid-19-detection-eda-data-augmentation#DICOM-data\n* https://www.kaggle.com/awsaf49/siim-covid-19-study-level-train-tpu/comments","metadata":{}}]}