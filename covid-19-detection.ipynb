{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Problem description\n\nFive times more deadly than the flu, COVID-19 causes significant morbidity and mortality. Like other pneumonias, pulmonary infection with COVID-19 results in inflammation and fluid in the lungs. COVID-19 looks very similar to other viral and bacterial pneumonias on chest radiographs, which makes it difficult to diagnose. This computer vision model for detection and localization of COVID-19 would help doctors provide a quick and confident diagnosis. As a result, patients could get the right treatment before the most severe effects of the virus take hold.\n\n\nCurrently, COVID-19 can be diagnosed via polymerase chain reaction to detect genetic material from the virus or chest radiograph. However, it can take a few hours and sometimes days before the molecular test results are back. By contrast, chest radiographs can be obtained in minutes. While guidelines exist to help radiologists differentiate COVID-19 from other types of infection, their assessments vary. In addition, non-radiologists could be supported with better localization of the disease, such as with a visual bounding box.\n\n\nIn this competition, the task is to identify and localize COVID-19 abnormalities on chest radiographs. In particular, categorization of the radiographs as negative for pneumonia or typical, indeterminate, or atypical for COVID-19.","metadata":{}},{"cell_type":"markdown","source":"**Categorization of the radiographs:**\n\n* NEGATIVE FOR PNEUMONIA - No lung opacities\n\n* TYPICAL APPEARANCE - Multifocal bilateral, peripheral opacities with rounded morphology, lower lung–predominant distribution\n\n* INDETERMINATE APPEARANCE - Absence of typical findings AND unilateral, central or upper lung predominant distribution\n\n* ATYPICAL APPEARANCE - Pneumothorax, pleural effusion, pulmonary edema, lobar consolidation, solitary lung nodule or mass, diffuse tiny nodules, cavity","metadata":{}},{"cell_type":"markdown","source":"**Input data:**\n\n* train_study_level.csv - the train study-level metadata, with one row for each study, including correct labels.\n* train_image_level.csv - the train image-level metadata, with one row for each image, including both correct labels and any bounding boxes in a dictionary format. Some images in both test and train have multiple bounding boxes.\n* sample_submission.csv - a sample submission file containing all image- and study-level IDs.\n* train folder - comprises 6334 chest scans in DICOM format, stored in paths with the form study/series/image\n* test folder - The hidden test dataset is of roughly the same scale as the training dataset. Studies in the test set may contain more than one label.","metadata":{}},{"cell_type":"markdown","source":"# Content table\n\n1. Importing the libraries\n2. Importing the datasets\n3. Data exploration\n4. Read Dicom files\n5. Feature engineering\n6. Making the model\n7. Compiling the model\n8. References","metadata":{}},{"cell_type":"markdown","source":"# Importing the libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sn\nimport pydicom as dicom # Dicom (Digital Imaging in Medicine) - medical image datasets, storage and transfer\nimport os\nfrom tqdm import tqdm # allows you to output a smart progress bar by wrapping around any iterable\nimport glob # retrieve files/pathnames matching a specified pattern\nimport pprint # pretty-print” arbitrary Python data structures\nimport ast # \nfrom pydicom.pixel_data_handlers.util import apply_voi_lut #\nimport wandb #\nfrom tensorflow.keras.applications import EfficientNetB0\n\npd.set_option('display.max_columns', 500)","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2021-06-11T14:29:33.242098Z","iopub.execute_input":"2021-06-11T14:29:33.24266Z","iopub.status.idle":"2021-06-11T14:29:33.2574Z","shell.execute_reply.started":"2021-06-11T14:29:33.242615Z","shell.execute_reply":"2021-06-11T14:29:33.256088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing the datasets","metadata":{}},{"cell_type":"code","source":"path = '/kaggle/input/siim-covid19-detection/'\ntrain_image_level = pd.read_csv(path + \"train_image_level.csv\")\ntrain_study_level = pd.read_csv(path + \"train_study_level.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:11.122168Z","iopub.execute_input":"2021-06-11T13:59:11.122472Z","iopub.status.idle":"2021-06-11T13:59:11.163961Z","shell.execute_reply.started":"2021-06-11T13:59:11.122446Z","shell.execute_reply":"2021-06-11T13:59:11.162883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data exploration","metadata":{}},{"cell_type":"markdown","source":"Let's have a look inside the train_image_level.","metadata":{}},{"cell_type":"code","source":"train_image_level.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:11.165935Z","iopub.execute_input":"2021-06-11T13:59:11.166508Z","iopub.status.idle":"2021-06-11T13:59:11.179715Z","shell.execute_reply.started":"2021-06-11T13:59:11.166453Z","shell.execute_reply":"2021-06-11T13:59:11.178441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_image_level.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:11.191691Z","iopub.execute_input":"2021-06-11T13:59:11.191993Z","iopub.status.idle":"2021-06-11T13:59:11.248181Z","shell.execute_reply.started":"2021-06-11T13:59:11.191964Z","shell.execute_reply":"2021-06-11T13:59:11.247195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 6334 unique values in the train_image_level dataframe.","metadata":{}},{"cell_type":"code","source":"train_study_level.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:11.249755Z","iopub.execute_input":"2021-06-11T13:59:11.250138Z","iopub.status.idle":"2021-06-11T13:59:11.262295Z","shell.execute_reply.started":"2021-06-11T13:59:11.2501Z","shell.execute_reply":"2021-06-11T13:59:11.26077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_study_level.describe()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:11.279571Z","iopub.execute_input":"2021-06-11T13:59:11.279852Z","iopub.status.idle":"2021-06-11T13:59:11.310477Z","shell.execute_reply.started":"2021-06-11T13:59:11.279826Z","shell.execute_reply":"2021-06-11T13:59:11.309188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 6054 rows in the train_study_level dataframe. The number of unique values in study dataframe differs from the unique values in the images dataframe. Let's check how many studies have more than 1 image linked.","metadata":{}},{"cell_type":"code","source":"train_study_level_key = train_study_level.id.str[:-6]\ntraining_set = pd.merge(left = train_study_level, right = train_image_level, how = 'right', left_on = train_study_level_key, right_on = 'StudyInstanceUID')\ntraining_set.drop(['id_x'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:11.312463Z","iopub.execute_input":"2021-06-11T13:59:11.312853Z","iopub.status.idle":"2021-06-11T13:59:11.348626Z","shell.execute_reply.started":"2021-06-11T13:59:11.312816Z","shell.execute_reply":"2021-06-11T13:59:11.34712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at these studies with multiple images:","metadata":{}},{"cell_type":"code","source":"training_set[training_set.groupby('StudyInstanceUID')['id_y'].transform('size') > 1].sort_values('StudyInstanceUID')","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:11.350736Z","iopub.execute_input":"2021-06-11T13:59:11.35138Z","iopub.status.idle":"2021-06-11T13:59:11.389323Z","shell.execute_reply.started":"2021-06-11T13:59:11.351335Z","shell.execute_reply":"2021-06-11T13:59:11.388264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read Dicom files","metadata":{}},{"cell_type":"markdown","source":"Function used to locate image from the path:","metadata":{}},{"cell_type":"code","source":"def extract_image(i):\n    path_train = path + 'train/' + training_set.loc[i, 'StudyInstanceUID']\n    last_folder_in_path = os.listdir(path_train)[0]\n    path_train = path_train + '/{}/'.format(last_folder_in_path)\n    img_id = training_set.loc[i, 'id_y'].replace('_image','.dcm')\n    print(img_id)\n    data_file = dicom.dcmread(path_train + img_id)\n    img = data_file.pixel_array\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:11.392274Z","iopub.execute_input":"2021-06-11T13:59:11.393003Z","iopub.status.idle":"2021-06-11T13:59:11.40023Z","shell.execute_reply.started":"2021-06-11T13:59:11.392947Z","shell.execute_reply":"2021-06-11T13:59:11.398994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Images and rectangles visualization**","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(3,3, figsize=(20,16))\nfig.subplots_adjust(hspace=.1, wspace=.1)\naxes = axes.ravel()\n\nfor row in range(9):\n    img = extract_image(row)\n    if (training_set.loc[row,'boxes'] == training_set.loc[row,'boxes']):\n        boxes = ast.literal_eval(training_set.loc[row,'boxes'])\n        for box in boxes:\n            p = matplotlib.patches.Rectangle((box['x'], box['y']),\n                                              box['width'], box['height'],\n                                              ec = 'r', fc = 'none', lw = 2.\n                                            )\n            axes[row].add_patch(p)\n    axes[row].imshow(img, cmap = 'gray')\n    axes[row].set_title(training_set.loc[row, 'label'].split(' ')[0])\n    axes[row].set_xticklabels([])\n    axes[row].set_yticklabels([])","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:11.402866Z","iopub.execute_input":"2021-06-11T13:59:11.403332Z","iopub.status.idle":"2021-06-11T13:59:21.275331Z","shell.execute_reply.started":"2021-06-11T13:59:11.403289Z","shell.execute_reply":"2021-06-11T13:59:21.274145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"**Opacity_Count** - Count the number of opacities in the image","metadata":{}},{"cell_type":"code","source":"Opacity_Count = training_set['label'].str.count('opacity')\ntraining_set['Opacity_Count'] = Opacity_Count.values","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:21.27699Z","iopub.execute_input":"2021-06-11T13:59:21.277462Z","iopub.status.idle":"2021-06-11T13:59:21.297251Z","shell.execute_reply.started":"2021-06-11T13:59:21.277417Z","shell.execute_reply":"2021-06-11T13:59:21.296233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Rectange_Area** - Sum of areas of rectangles - assumption : the bigger the rectangle - the bigger the opacity","metadata":{}},{"cell_type":"code","source":"image_rectangles_areas = []\n\nfor row in range(6334):#len(training_set.index)):\n    image_rectangles_area_sum = 0\n    rectangle_area = 0\n    if (training_set.loc[row,'boxes'] == training_set.loc[row,'boxes']):\n        boxes = ast.literal_eval(training_set.loc[row,'boxes'])\n        for box in boxes:\n            rectangle_area = box['width'] * box['height']\n            image_rectangles_area_sum = image_rectangles_area_sum + rectangle_area\n        image_rectangles_areas.append(image_rectangles_area_sum)\n    else:\n        image_rectangles_area_sum = image_rectangles_area_sum + rectangle_area\n        image_rectangles_areas.append(image_rectangles_area_sum)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:21.298698Z","iopub.execute_input":"2021-06-11T13:59:21.299289Z","iopub.status.idle":"2021-06-11T13:59:21.711778Z","shell.execute_reply.started":"2021-06-11T13:59:21.299248Z","shell.execute_reply":"2021-06-11T13:59:21.710658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set['Rectangle_Area'] = image_rectangles_areas","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:21.713321Z","iopub.execute_input":"2021-06-11T13:59:21.71378Z","iopub.status.idle":"2021-06-11T13:59:21.722474Z","shell.execute_reply.started":"2021-06-11T13:59:21.71373Z","shell.execute_reply":"2021-06-11T13:59:21.720984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Creating buckets - rectangle areas**","metadata":{}},{"cell_type":"markdown","source":"First see the distribution of the rectangle areas","metadata":{}},{"cell_type":"code","source":"training_set['Rectangle_Area'] = round(training_set['Rectangle_Area'],2)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:21.723713Z","iopub.execute_input":"2021-06-11T13:59:21.724356Z","iopub.status.idle":"2021-06-11T13:59:21.737477Z","shell.execute_reply.started":"2021-06-11T13:59:21.724297Z","shell.execute_reply":"2021-06-11T13:59:21.7365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_set['Rectangle_Area']","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:21.742209Z","iopub.execute_input":"2021-06-11T13:59:21.742608Z","iopub.status.idle":"2021-06-11T13:59:21.754122Z","shell.execute_reply.started":"2021-06-11T13:59:21.74258Z","shell.execute_reply":"2021-06-11T13:59:21.752826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#pd.qcut(training_set['Rectangle_Area'], q = 4)\n\n#training_set.boxplot(by = \"Negative for Pneumonia\",column = ['Rectangle_Area'],grid = True, layout=(1, 1))\n\ncut_labels_4 = ['0', '<1e6', '<2e6', '<4e6', '<8e6']\ncut_bins = [-1, 0, 1000000, 2000000, 4000000, 8000000]\ntraining_set['Rectangle_Area_Bin'] = pd.cut(training_set['Rectangle_Area'], bins=cut_bins, labels=cut_labels_4)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:21.756531Z","iopub.execute_input":"2021-06-11T13:59:21.757157Z","iopub.status.idle":"2021-06-11T13:59:21.768486Z","shell.execute_reply.started":"2021-06-11T13:59:21.757095Z","shell.execute_reply":"2021-06-11T13:59:21.767372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\n\nplt.figure(figsize = (16, 14))\nsn.set(font_scale = 1.2)\nsn.set_style('ticks')\n\nfor i, column in enumerate(columns):\n    plt.subplot(3, 3, i + 1)\n    sn.countplot(data = training_set, x = 'Rectangle_Area_Bin', hue = column, palette = ['#d02f52',\"#55a0ee\"])\n    \nsn.despine()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:21.772358Z","iopub.execute_input":"2021-06-11T13:59:21.772775Z","iopub.status.idle":"2021-06-11T13:59:22.495343Z","shell.execute_reply.started":"2021-06-11T13:59:21.772747Z","shell.execute_reply":"2021-06-11T13:59:22.493991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#columns = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\nopacity = sorted(list(training_set['Rectangle_Area_Bin'].value_counts().index))\n\nfor i in opacity:\n    Count_Series = training_set[training_set['Rectangle_Area_Bin'] == i].iloc[:,[1, 2, 3, 4]].sum()\n    fig = plt.figure(figsize=(12,3))\n    sn.barplot(x = Count_Series.index, y = Count_Series.values/sum(training_set['Rectangle_Area_Bin'] == i))\n    plt.title('Rectangle_Area_Bin : {} '.format(i))\n    plt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:22.497313Z","iopub.execute_input":"2021-06-11T13:59:22.497737Z","iopub.status.idle":"2021-06-11T13:59:23.223401Z","shell.execute_reply.started":"2021-06-11T13:59:22.497698Z","shell.execute_reply":"2021-06-11T13:59:23.222102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rectangle area and opacity count","metadata":{}},{"cell_type":"code","source":"#columns = ['Negative for Pneumonia', 'Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance']\nopacity = sorted(list(training_set['Opacity_Count'].value_counts().index))\n\nfor i in opacity:\n    Count_Series = training_set[training_set['Opacity_Count'] == i].iloc[:,[1, 2, 3, 4]].sum()\n    fig = plt.figure(figsize=(12,3))\n    sn.barplot(x = Count_Series.index, y = Count_Series.values/sum(training_set['Opacity_Count'] == i))\n    plt.title('OpacityCount : {} '.format(i))\n    plt.plot()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:23.225124Z","iopub.execute_input":"2021-06-11T13:59:23.225596Z","iopub.status.idle":"2021-06-11T13:59:24.349022Z","shell.execute_reply.started":"2021-06-11T13:59:23.225543Z","shell.execute_reply":"2021-06-11T13:59:24.347776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TBD**: Position of the rectangle by quadrants (4 bins - 4 quadrants)","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Image metadata**","metadata":{}},{"cell_type":"code","source":"training_paths = []\ntrain_directory = \"../input/siim-covid19-detection/train/\"\n\nfor sid in tqdm(training_set['StudyInstanceUID']):\n    training_paths.append(glob.glob(os.path.join(train_directory, sid +\"/*/*\"))[0])\n\ntraining_set['path'] = training_paths","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:24.350951Z","iopub.execute_input":"2021-06-11T13:59:24.351402Z","iopub.status.idle":"2021-06-11T13:59:50.363476Z","shell.execute_reply.started":"2021-06-11T13:59:24.351359Z","shell.execute_reply":"2021-06-11T13:59:50.362236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The pixel values are in the range of 0 to 255. It is easier for us to normalize the data between 0 to 1 and we can do that just by dividing our train and test set by 255.","metadata":{}},{"cell_type":"code","source":"voi_lut=True\nfix_monochrome=True\n\ndef dicom_dataset_to_dict(filename,func):\n    \"\"\"Credit: https://github.com/pydicom/pydicom/issues/319\n               https://www.kaggle.com/raddar/convert-dicom-to-np-array-the-correct-way\n    \"\"\"\n    \n    dicom_header = dicom.dcmread(filename) \n    \n    #====== DICOM FILE DATA ======\n    dicom_dict = {}\n    repr(dicom_header)\n    for dicom_value in dicom_header.values():\n        if dicom_value.tag == (0x7fe0, 0x0010):\n            #discard pixel data\n            continue\n        if type(dicom_value.value) == dicom.dataset.Dataset:\n            dicom_dict[dicom_value.name] = dicom_dataset_to_dict(dicom_value.value)\n        else:\n            v = _convert_value(dicom_value.value)\n            dicom_dict[dicom_value.name] = v\n      \n    del dicom_dict['Pixel Representation']\n    \n    if func != 'metadata_df':\n        #====== DICOM IMAGE DATA ======\n        # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n        if voi_lut:\n            data = apply_voi_lut(dicom_header.pixel_array, dicom_header)\n        else:\n            data = dicom_header.pixel_array\n        # depending on this value, X-ray may look inverted - fix that:\n        if fix_monochrome and dicom_header.PhotometricInterpretation == \"MONOCHROME1\":\n            data = np.amax(data) - data\n        data = data - np.min(data)\n        data = data / np.max(data)\n        modified_image_data = (data * 255).astype(np.uint8)\n    \n        return dicom_dict, modified_image_data\n    \n    else:\n        return dicom_dict\n\ndef _sanitise_unicode(s):\n    return s.replace(u\"\\u0000\", \"\").strip()\n\ndef _convert_value(v):\n    t = type(v)\n    if t in (list, int, float):\n        cv = v\n    elif t == str:\n        cv = _sanitise_unicode(v)\n    elif t == bytes:\n        s = v.decode('ascii', 'replace')\n        cv = _sanitise_unicode(s)\n    elif t == dicom.valuerep.DSfloat:\n        cv = float(v)\n    elif t == dicom.valuerep.IS:\n        cv = int(v)\n    else:\n        cv = repr(v)\n    return cv\n\n","metadata":{"execution":{"iopub.status.busy":"2021-06-11T13:59:50.365266Z","iopub.execute_input":"2021-06-11T13:59:50.365698Z","iopub.status.idle":"2021-06-11T13:59:50.380907Z","shell.execute_reply.started":"2021-06-11T13:59:50.365657Z","shell.execute_reply":"2021-06-11T13:59:50.379499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting the dictionary data to the dataframe and dropping the columns not needed","metadata":{}},{"cell_type":"code","source":"metadata = []\n\nfor filename in training_set.path:\n    try:\n        data_di = dicom_dataset_to_dict(filename,'metadata_df')\n        metadata.append(data_di)\n    except:\n        continue\n\ndicom_data_df = pd.DataFrame(metadata)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-11T13:59:50.382914Z","iopub.execute_input":"2021-06-11T13:59:50.383585Z","iopub.status.idle":"2021-06-11T14:16:30.773119Z","shell.execute_reply.started":"2021-06-11T13:59:50.383546Z","shell.execute_reply":"2021-06-11T14:16:30.771968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dicom_data_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:16:30.774698Z","iopub.execute_input":"2021-06-11T14:16:30.775152Z","iopub.status.idle":"2021-06-11T14:16:30.810095Z","shell.execute_reply.started":"2021-06-11T14:16:30.775098Z","shell.execute_reply":"2021-06-11T14:16:30.808849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dicom_data_df.drop(['Specific Character Set', 'SOP Class UID','SOP Instance UID','Study Date','Study Time','Accession Number','Patient ID','Accession Number','Rows','Columns'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:16:30.811793Z","iopub.execute_input":"2021-06-11T14:16:30.812504Z","iopub.status.idle":"2021-06-11T14:16:30.860524Z","shell.execute_reply.started":"2021-06-11T14:16:30.812458Z","shell.execute_reply":"2021-06-11T14:16:30.859218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get the metadata information as new columns in an existing dataframe","metadata":{}},{"cell_type":"code","source":"training_set_merged = pd.merge(left = training_set, right = dicom_data_df, how = 'left', left_on = 'StudyInstanceUID', right_on = 'Study Instance UID')\ntraining_set_merged","metadata":{"execution":{"iopub.status.busy":"2021-06-11T14:16:30.862155Z","iopub.execute_input":"2021-06-11T14:16:30.863027Z","iopub.status.idle":"2021-06-11T14:16:30.950868Z","shell.execute_reply.started":"2021-06-11T14:16:30.862968Z","shell.execute_reply":"2021-06-11T14:16:30.949864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TBD**: Outliers and irregularities in the data","metadata":{}},{"cell_type":"markdown","source":"# Making the model","metadata":{}},{"cell_type":"markdown","source":"EfficientNet is used:\n\nThe EfficientNet scaling method uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients. For example, if we want to use  times more computational resources, then we can simply increase the network depth by , width by , and image size by , where  are constant coefficients determined by a small grid search on the original small model.\n\nThe compound scaling method is justified by the intuition that if the input image is bigger, then the network needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on the bigger image.\n\nThe base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2, in addition to squeeze-and-excitation blocks.","metadata":{}},{"cell_type":"code","source":"# The only thing that needs to be declared inside is model creation. \n# If you use use Keras .fit() instead of custom training then model.compile() has to be inside as well.\nwith strategy.scope():","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compiling the model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n* https://github.com/pydicom/pydicom/issues/319\n* https://www.kaggle.com/songseungwon/siim-covid-19-detection-10-step-tutorial-1\n* https://www.kaggle.com/ruchi798/siim-covid-19-detection-eda-data-augmentation#DICOM-data","metadata":{}}]}